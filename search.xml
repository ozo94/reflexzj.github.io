<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[post&get 方法]]></title>
      <url>%2F2017%2F03%2F15%2Fpost_get%2F</url>
      <content type="text"><![CDATA[HTTP 定义了与服务器交互的不同方法，最基本的方法是 GET 和 POST（Ajax开发，关心的只有GET请求和POST请求）。 GET与POST方法有以下区别： 在客户端，Get方式在通过URL提交数据，数据在URL中可以看到；POST方式，数据放置在HTML HEADER内提交。 GET方式提交的数据最多只能有1024字节，而POST则没有此限制。 安全性问题。使用 Get 的时候，参数会显示在地址栏上，而 Post 不会。所以，如果这些数据是中文数据而且是非敏感数据，那么使用 get；如果用户输入的数据不是中文字符而且包含敏感数据，那么还是使用 post为好。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python异常处理方法]]></title>
      <url>%2F2017%2F03%2F11%2Ftry%E8%AF%AD%E5%8F%A5%2F</url>
      <content type="text"><![CDATA[1.捕获所有异常123456try: a=b b=c except Exception,e: print Exception,&quot;:&quot;,e 2.采用traceback模块查看异常1234567891011121314151617181920#引入python中的traceback模块，跟踪错误import traceback try: a=b b=c except: traceback.print_exc()#也可以将异常存储到日志文件中去import tracebacktry: a=b b=c except: f=open("c:log.txt",'a') traceback.print_exc(file=f) f.flush() f.close() 3.采用sys模块回溯最后的异常12345678#引入sys模块import sys try: a=b b=c except: info=sys.exc_info() print info[0],":",info[1]]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[静态网页爬虫抓取]]></title>
      <url>%2F2016%2F12%2F05%2F%E9%9D%99%E6%80%81%E7%BD%91%E9%A1%B5%E7%88%AC%E8%99%AB%E6%8A%93%E5%8F%96%2F</url>
      <content type="text"><![CDATA[爬虫是一种自动抓取互联网信息的程序，可以取互联网中有价值的数据。传统的人工方式，通过URL之间的联系来获取网页，覆盖面小，效率低下。开发一个最简易的爬虫（爬去静态网页，没有使用框架，效率很低，仅作为对爬虫的入门了解）源码地址：https://github.com/reflexzj/simple-crawler.git 一、概述简单的爬虫组成 URL管理器 网页下载器 网页解析器这三个部分组成 给出三个部件协同工作流程 二、url管理器 为什么需要urlManager这个组件？ 防止重复抓取、循环抓取（最坏情况：两个URL之间相互指向） 添加新的URL时需要判断 URL管理的基本功能范围 实现方案 使用集合将待爬取的url放入集合中 python中的set()方法可以去除重复元素 关系型数据库 通过表和相应的标志位来实现这个功能 缓存数据库 eg:redis,使用两个数据的方案（大型公司的选用方式）个人可以使用电脑的内存或者是关系型数据库 三、网页下载器将互联网上URL对应的网页下载到本地的工具 工具 urllib2 Python官方的基础模块： 支持直接的URL下载 向网页提交一些需要用户输入的数据 需要登陆网页的cookie处理 代理访问功能 requests 第三方插件，提供更为强大的功能 urllib2用法 1.将URL传送给urllib2.urlopen()方法123456789import urllib2 #直接请求 response = urllib2.urlopen('http://www.baidu.com') #获取状态码，如果是200，则表示获取成功 print response.getcode() #读取内容 contt = response.read() 2.添加data、http header 增强处理：向服务器提交需要用户输入的数据 将url、data、header等信息传给request，将request对象作为参数发送网页请求123456789101112import urllib2 #创建request对象 request = urllib2.request(url) #添加数据（'a'这个数据项的值为'1'） request.add_data('a','1') #添加http的header（伪装成Mozilla浏览器） request.add_header('USer-Agent','Mozilla/5.0') #发送请求获取结果 response = urllib2.urlopen(request) 3.添加特殊场景的处理登陆访问：HTTPCookieProcesser 代理访问：ProxyHandler 协议加密：HTTPSHandler URL之间相互跳转：HTTPRedirectHandler …..eg：增强cookie的处理123456789101112import urllib2, cookielib #创建cookie容器 cj = cookielib.CookieJar() #将cookieJar作为一个参数生成对应的Handler，将此Handler传给Opener opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj)) #增强处理器，为urllib2安装opener urllib2.install_opener(opener) #此时使用带有cookie的urllib2来访问网页 response = urllib2.urlopen("http://www.baidu.com/") 四、网页解析器种类介绍 1.模糊匹配解析正则表达式 将整个网页文档当成一个字符串，使用模糊匹配的方式提取数据 直观，面对复杂内容，效率底下 2.结构化解析将整个网页文档下成一个文档对象模型树DOM(document Object Model) 解析工具： 1.html.parser自带的解析工具 2.Beautiful Soup可已使用html.parser以及Ixml来作为其的解析器 3.Ixml Beautiful Soup 安装 文档参考地址：https://www.crummy.com/software/BeautifulSoup/bs4/doc/在windows环境下通过python工具包自带的pip工具来安装pip程序在Scripts文件中，检查是否安装 cd python_27\Scripts安装 pip install beautifulsoup4 用法 原理根据下载好的Html网页创建一个BeatifulSoup对象，文档则会变为相应的DOM树 代码实现（1）创建BeautifulSoup对象 1234from bs4 import BeautifulSoup #根据HTML网页字符串创建BeautifulSoup对象 #对象中的参数分别表示为HTML文档字符串、HTML解析器、指定文档的编码 soup= BeautifulSoup(html_doc,'html.parser',from_encoding='utf8') （2）搜索节点（find_all,find） 这两个方法具有相同的参数：d_all(name,attrs,string) find_all() 搜索所有满足条件的节点 find() 搜索第一个满足条件的节点 方法中的三个参数代表搜索的三个方向，分为名称、属性、内容 123456789 #查找所有标签为a的节点soup.find_all('a')#查找所有标签为啊，链接符合/view/123.htm形式的节点(也可以传入正则表达式)soup.find_all('a',href='/view/123.htm')soup.find_all('a',href=re.compile(r'/view/\d+\.htm'))#查找所有标签为div，class为abc，文字为Python的节点(class为python关键字，加下划线避免冲突)soup.find_all('div',class_='abc',string='Python') （3）访问节点信息 12345678910 #得到节点：&lt;a href='1.html'&gt;Python&lt;/a&gt;#获取查找到的节点标签名称node.name#获取查找到的a节点的href属性node['href']#获取查找到的a节点的链接文字node.get_text()]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[__name__ = '__main__' 的作用]]></title>
      <url>%2F2016%2F10%2F05%2Fmain%E5%87%BD%E6%95%B0%E5%88%86%E6%9E%90%2F</url>
      <content type="text"><![CDATA[给一个模块12345#module.pydef main(): print "we are in %s"%__name__if __name__ == '__main__': main() 这个函数定义了一个main函数，执行后打印出”we are in main“,说明if语句中的内容被执行了，调用了main() 从另外一个模块中调用main()方法123#anothermodle.pyfrom module import mainmain() 执行的结果是：we are in module 没有显示”we are in main“,也就是说模块name = ‘main‘ 下面的函数没有执行。 分析直接执行某个.py文件的时候，该文件中那么”name == ‘main‘“是True,但是如果从另外一个.py文件通过import导入该文件的时候，这时name的值就是这个py文件的名字而不是main。 这个功能还有一个用处：调试代码的时候，在”if name == ‘main‘“中加入一些的调试代码，可以让外部模块调用的时候不执行调试代码，但是如果想排查问题的时候，直接执行该模块文件，调试代码能够正常运行。]]></content>
    </entry>

    
  
  
</search>
