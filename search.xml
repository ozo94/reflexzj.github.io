<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[数据挖掘流程初步学习（LDA-SVM）]]></title>
      <url>%2F2017%2F07%2F09%2FLDA-SVM%2F</url>
      <content type="text"><![CDATA[基本流程 数据挖掘往小处说，就是对数据的处理归纳，得出有用信息的一个过程。通过各种机器学习算法，模型训练的过程中，发现即使用的是相同的算法，相同的参数，得到的数据模型往往是不稳定的。 项目主要用到的包gensim的官方地址：http://radimrehurek.com/gensim/index.html) 中文文档的对照地址：http://blog.csdn.net/questionfish/article/details/46715795# 项目地址：https://github.com/reflexzj/data_mining_learning 本次的主要流程图如下： 数据的爬取 爬取数据时，选择较为干净的数据源，可以将数据整理成对应的结构化数据 本次，走完了一个简单的数据挖掘流程，主要爬取了知网上的九类学科的期刊文档。数据预处理 质量较高的句子信息，考虑jieba分词模型李：中英文的分类、去除标点符号、去除停用词 高质量的结构化数据，将数据整理为csv模式的数据结构生成语料库 对所有文档中所有的不同的单词进行统计，整理到一个字典中去（统计维度） 产生稀疏向量文档特征选择 根据词再本篇以及所有的文档中的词频来计算出特征值LDA模型建立 主题模型，反馈出每篇文章中不同主题的比重 选择多少的主题？SVM模型选择 SVM模型的使用主要涉及到模型参数的调整 数据预处理1. 文件读取 数据以多级文件夹的形式来分类存储的，首先我们要将这些文本读入到一个txt文件夹中去（不管一个基础的txt源文件中有多少行，统一到result.txt文件中后，只对应到一行） 可以一次性读取了所有的文本内容，也可以读取单个文件夹下的所有内容（1）读取文件路径下的所有文件，以子文件夹为单位，逐个完成读取os.walk方法以最小的子文件夹为单位，读取该文件夹下所有的内容（文件路径，好像不区分正反斜杠，默认反斜杠）12345rootdir = '../data_01/'for parent, dirnames, filenames in os.walk(rootdir): for filename in filenames: path = os.path.join(parent,filename) print path （2）遍历指定目录，显示目录下的所有文件名123456789def eachFile(filepach): files = [] pathDir = os.listdir(filepach) for allDir in pathDir: # child = os.path.join("%s%s" %(filepach, allDir)) # print child.decode('gbk') files.append(filepach+allDir) print files return files 读取多层的目录结构下的文件 循环读取结构目录下的所有文件（清楚目录结构，目录的结构层次不能深）12345678910111213141516171819def get_result(filepach): # 获取文件夹的根目录, 将数据文本的rar文件解压到制定目录下（获得了分类好的子文件夹） datas = [] datas = eachFile(filepach) # 获取每个文件夹下的文件目录 fopen = open('data/result.txt', 'w') for each in datas: filepach = each + '/' files = eachFile(filepach) # 遍历所有的每个文件夹下的所有txt文件 for filename in files: data = handle(filename) # print data if data: fopen.write(data + '\n') print filename + 'has done' 2. 分词 中文分词主要使用jieba来完成分词，属于自然语言处理的部分。 jieba中文分词模式介绍参考地址：http://blog.csdn.net/xiaoxiangzi222/article/details/53483931 主要针对完整的中文句子，提取对应的分词信息 获取汉字内容： 去除英文、标点符号等信息 去除停用词： 根据停用词表去除句子中的停用词 去除重复词: 在不同的模型中，词频是否是考量因素是不一致的，这一步骤需要考虑是否执行 （1）获取当前文本中的汉字内容12345def filter_chinese_character(self, string): regex = u"([\u4e00-\u9fa5]+)" pattern = re.compile(regex) results = pattern.findall(string) return ''.join(results) （2）移除当前文本中的停用词1234567891011def remove_stop_word(self, string): # 生成停用词的字典键值列表 stop_words = &#123;&#125;.fromkeys([line.rstrip() for line in open(self.url_stopwords)]) all_words = pseg.cut(string) filter_words = '' for word in all_words: if ((word.flag == 'n' or word.flag == 'v') and len(word.word) &gt; 1): if word.word not in stop_words: filter_words += word.word.encode('utf-8') + ',' return filter_words （3）移除当前文本中的重复词12345def remove_repeat_word(self, string): tmp_list = string.split(',') word_set = list(set(tmp_list)) result = ','.join(word_set) return result.strip(',') 3. 格式最终文件处理成如下格式 建立语料库 主要使用了gensim这个包来进行语料提取工作 ，对原始语料库的建立包含了如下的四个步骤 英文的官方地址：http://radimrehurek.com/gensim/tut1.html) （1）预处理文件的文本格式处理 处理文本中的单词（停用词，低频词可以后面处理），这是dictionary接收的标准模式 在前文的预处理中，已经将单词提出来了，直接读入就好了12345def to_texts(file): datas = open(file, 'r') texts = [[word for word in data.split(',')] for data in datas] return texts （2）维度统计，生成对应的词典 生成的词典还要去除停用词和低频词 1234567891011121314151617181920def tokens(texts): dictionary = corpora.Dictionary(texts) dictionary.save('temp/tokens.dict') # 停用词 # stop_ids = [dictionary.token2id[stopword] for stopword in stoplist # if stopword in dictionary.token2id] # 出现一次的低频词 once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq == 1] # 删除停用词和低频词，并去除不连续的缺口 dictionary.filter_tokens(once_ids) dictionary.compactify() # for index,data in dictionary.token2id.items(): # print index,data return dictionary （3）产生稀疏文档向量 很大的稀疏矩阵，统计每行对应的单词（在步骤2，词典中的）出现的频率 (id, fre)，id 对应字典中的单词，fre 为该单词出现的频率 词库存入备用，将前面的文本操作于后面的模型训练过程分开，方便调用 12345678910def to_vectors(dictionary, texts): corpus = [dictionary.doc2bow(text) for text in texts] # 存入硬盘备用 corpora.MmCorpus.serialize('temp/vectors.mm', corpus) # for data in corpus: # print data return corpus （4）存储为不同的语料库格式载入语料库：从步骤3中的备用文件读入，存储为不同的语料库格式12345678def corpus_formate(corpus): #存储为不同的语料 corpora.SvmLightCorpus.serialize('data/corpus.svmlight', corpus) corpora.BleiCorpus.serialize('data/corpus.lda-c', corpus) corpora.LowCorpus.serialize('data/corpus.low', corpus) #载入语料库 corpus = corpora.MmCorpus('temp/vectors.mm') 特征选择什么是IF-IDF?IF-IDF(Term Frequency-Inverse Document Frequency, 词频-逆文件频率) 是一种用于资讯检索与资讯探勘的常用加权技术。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。 字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。 一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章. 具体原理参考地址：http://blog.csdn.net/zrc199021/article/details/53728499 使用TF_IDF实现特征选择 模型调用词库 建立特征词和值之间的映射，并逆序，根据一定的比例取 每个文档建立一个关系集，按一定比列提取 gensim官方地址：http://radimrehurek.com/gensim/models/tfidfmodel.html 代码123ddef to_Tfidmodle(corpus): tfidf = models.TfidfModel(corpus) tfidf.save('data/corpus.tfidf_model') LDA模型 LDA是一种主题模型：根据前文给出的语料库（不是特征选择的IF_IDF文件），设定参数，调整需要分类的主题数目LDA主题用在聚类模型中，本文使用LDA，是基于词带模型，为训练数据训练出合适的属性（主题数） 那一个训练样本的主题数该如何选择？参考地址：https://www.zhihu.com/question/32286630 两种调用LDA的方法：（1）使用gensim中自带函数一键处理12from gensim import modelslda = models.LdaModel(corpus, num_topics= 9) （2）使用默认的lda库，需要进行大量的预处理操作 以字典中所有单词作横坐标，不同的文本号为纵坐标，建立包含大量0的稀疏矩阵 使用fit_tansform()函数进行主题模型值得计算，返回的是一个矩阵 对该举证进行归一化处理 代码12345678910111213141516171819202122232425262728def to_ldaModel(corpus, topic_num): start = time.time() lda = models.LdaModel(corpus, num_topics= topic_num) # 打印到csv文件中去，观察文件的内容格式。发现主题模型每次训练完之后，得到的主题分布结果是不一致的 lda_file = open('data/lda_file.csv', 'w') train_x = open('data/train_x.csv', 'w') for each in lda[corpus]: lda_file.write(str(each)+ '\n') # 将主题模型存储为对应的train_x,没有主题分布的地方补充为0 x_each = [] attr_no = 0 for attribute in each: for key in range(attr_no, int(attribute[0])): x_each.append(0.0) x_each.append(attribute[1]) attr_no = int(attribute[0])+1 for key in range(attr_no, topic_num): x_each.append(0) train_x.write(str(x_each)[1:-1]+'\n') end =time.time() print '---------- build lda model ---------- ', '\ntopic_num: ', topic_num, \ '\ntime cost: %0.2f' % (end-start) SVM模型 sklearn中的给出了对应的SVC模型官方文档参考地址：http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html 参数说明：（重要点）kernel：svm核函数的选择参考地址：https://www.zhihu.com/question/21883548 本篇中，SVM模型LDA模型的联系： LDA是一个聚类模型，往往是用以训练没有标签的数据，所以得到的主题模型往往是不稳定的，同参同实现方法往往会面临不同的结果 LDA给庞大的文本打了标签（词典模型），给出了主题分布。而这个主题分布就是投入SVM中的属性值（train_X），而（train_Y）则是最初爬取的文本后，自己做的分类（九类期刊内容） 以下主要涉及模型的调用（包括训练集、测试集的划分，交叉验证法的选择）12345678910111213141516171819202122232425262728293031def do_svm(x_file, y_file, rate, cv): start = time.time() x = pd.read_csv(x_file, header=None) y = pd.read_csv(y_file, header=None) # 样本的标准化处理（这里，标签Y并不需要标准化处理） scaler = preprocessing.StandardScaler().fit(x) x_transform = scaler.transform(x) svc_model = svm.SVC(kernel= 'linear', C=1) # （1）传统的验证集，测试集的划分 # train_x, test_x, train_y, test_y = train_test_split(x_transform, y, test_size=rate, random_state= 11) # svc_model.fit(train_x, train_y) # result = svc_model.score(test_x, test_y) # 详细的结果预测方法 # prediction = svc_model.predict(test_x) # result = metrics.classification_report(test_y, prediction) # （2）交叉验证法（cross_val_score）,函数中接收ndarray的数据格式 y = y.values[: , 0] svc_model = svm.SVC(kernel= 'linear', C=1) result = cross_val_score(svc_model, x_transform, y, cv=cv) end = time.time() print '---------- svm model leaning ---------- \ntime cost: %0.2f' % (end-start), \ "\nAccuracy: %0.2f (+/- %0.2f)" % (result.mean(), result.std() * 2), '\n' 后续总结1. 数据处理目标主要集中在处理csv文件上，使用到的包主要包括：pandas, numpy pands主要体现在csv文件度，属性提取上面（dataform） numpy主要关于数组方面的处理 预处理归一化、标准化、正则化之间的选择与区别链接：http://blog.csdn.net/power0405hf/article/details/53456162 2. 交叉验证 _sklearn里的指向文档：http://scikit-learn.org/dev/modules/cross_validation.html#cross-validation_ 其中傻瓜式用法：cross_val_score(基于Kfold 和 StratifiedKFold这两者算法) 上述算法的引用，中文文档参考：http://blog.csdn.net/ztchun/article/details/71169530 所用的sklearn包中的几个交叉验证大方法，对投入的数据格式有要求通过用pands 和 numpy 主要产生以下两种数据格式12345x = pd.read_csv(x_file, header=None) scaler = preprocessing.StandardScaler().fit(x)x_transform = scaler.transform(x)print type(x_transform), type(x) 结果： 总结： 用numpy生成的数组和标准化后的数据都是 ndarray pandas 直接读的是dataframe流 train_test_split：两种格式都能接收 cross_val_score（KFold or StratifiedKFold）：只接收ndarray格式 3. 调参为什么要调参？ 所有调用的API中包含了大量的引用参数，我们要根据数据集的分布模式来确定参数值 给出个例子： 当我们在做LDA主题聚类的时候，主题数目应该设为多少才合适 把这些主题投入SVM后，根据现有的标签来做验证，更具结果就可以判断出来了 这就是监督学习的有点（典型的聚类无监督，则比较难了，标注难） 疑问： 当我不断调整LDA主题数时，一定能够找到最有的参数值么？ 调参的过程中，这个准确率的图可以近似看成一个平滑曲线么？在某一个区间能够取到极值？（曲线图是连续曲线还是散列点….） 有结果显示，随着主题数，准确率会上升？]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[熟悉python基本算法]]></title>
      <url>%2F2017%2F03%2F29%2Fpython%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86%2F</url>
      <content type="text"><![CDATA[从基础算法开始，一道道的刷过去，争取早日熟悉python的语法，持续更新……主要的题目来源：leetcode( https://leetcode.com/ )。 当然，在牛客网上也有对应的leetcode题库。 字符串字符串逆序明显的字符串切片（list也可以进行切片操作）备注一下倒叙切片的语法 s[-4:-1]倒数第一个元素的序号为-1（符合数学习惯）1234567class Solution(object): def reverseString(self, s): """ :type s: str :rtype: str """ return s[::-1] 进制操作基本的进制函数二进制：bin(num)八进制：oct(num)十六进制：hex(num) 将对应的进制的数转为十进制12#使用方法： int(str, num)int('0x10', 16) &gt;&gt;&gt;16 汉明距离进行二进制转换后的异或位数（同位不同值即为一个距离）1bin(x^y).count('1') 二进制的非操作要想’0’,’1’发生互换，对应位置与’1’进行异或操作即可难点在于： 如何产生与num二进制转化后，同样长度的’1’ 可以不断将i=1 左移（相当于乘以2，可以循环与二进制转化后相同数目的次数） 最后产生：i-1= 10..0-1= 011..1 12345def findComplement(self, num): i = 1 while i &lt;= num: i = i &lt;&lt; 1 return (i - 1) ^ num list以及dict的联合使用list相邻元素之间的比较判断list的相邻的两个元素之间的关系使用另外一个列表协同处理，设置中间列表tmp=[] 123456789st=[]for x in range(len(st)): judge(st[x], s[x+1])for x in st: while len(tmp) and judge(tmp[-1], x): ... pass tmp.append(x)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openedx的基本配置]]></title>
      <url>%2F2017%2F03%2F08%2Fopenedx%E7%9A%84%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE%2F</url>
      <content type="text"><![CDATA[搭建好openedx后，我们需要对edx的一些基本功能进行配置 edx文档参考手册官方网站（包括API调用、开发者手册）：http://docs.edx.org/ openedx的配置主要参考安装配置手册：http://edx.readthedocs.io/projects/edx-installing-configuring-and-running/en/latest/index.html 原博客参考：https://shanbin.name/openedx-concise-configuration-guide/ 配置SMTP主要涉及/edx/app/edxapp/下的4个json文件 lms.env.json 1234567891011121314151617#设置成你的smtp邮箱“DEFAULT_FROM_EMAIL”: “your email”, #内容修改成你的smtp主机，比如你的邮箱设置的QQ邮箱则是smtp.qq.com“EMAIL_HOST”: “smtp.qq.com”, #端口一般是25 “EMAIL_PORT”: 25, #你的平台域名，可填ip“LMS_BASE”: “example.com”, #内容修改成你的edX平台名字“PLATFORM_NAME”: “OPENEDX”, #域名或ip,激活邮箱时调用 “SITE_NAME”: “localhost”, cms.env.json 12345678910#设置成你的smtp邮箱“BULK_EMAIL_DEFAULT_FROM_EMAIL”: “your email”, #设置成你的smtp邮箱“DEFAULT_FROM_EMAIL”: “your email”, “EMAIL_HOST”: “smtp.qq.com”, “EMAIL_PORT”: 25, “LMS_BASE”: “example.com”, “SITE_NAME”: “localhost”, lms.auth.json 12345#smtp邮箱密码“EMAIL_HOST_PASSWORD”: “password”, #邮箱“EMAIL_HOST_USER”: “email”, cms.authe.json 12345#smtp邮箱密码“EMAIL_HOST_PASSWORD”: “password”, #邮箱 “EMAIL_HOST_USER”: “email”, 管理命令 添加管理员用户找到 .manage.py 所在的目录文件执行命令 1sudo -u www-data /edx/bin/python.edxapp ./manage.py lms --settings=aws createsuperuser 列出所有的manage.py参数命令 12sudo -u www-data /edx/app/edxapp/venvs/edxapp/bin/python/edx/app/edxapp/edx-platform/manage.py lms –-settings=aws helpn 重启edxapp 1sudo /edx/bin/supervisorctl restart edxapp: 查看服务器状态 1sudo /edx/bin/supervisorctl status 文件编译在edxapp账户下对lms模块进行编译，编译结束后要重启edxapp(cms编译时进行对应修改即可) 切换edxapp账户 1sudo -H -u edxapp bash 编译静态文件 123source /edx/app/edxapp/edxapp_envcd /edx/app/edxapp/edx-platformpaver update_assets lms --settings=aws 重启edxapp 1sudo /edx/bin/supervisorctl restart edxapp:]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openedx搭建笔记]]></title>
      <url>%2F2017%2F03%2F01%2Fopenedx%E6%90%AD%E5%BB%BA%2F</url>
      <content type="text"><![CDATA[本次小组实训任务，在开源项目openedx的基础上，搭建自己的MOOC平台openedx这个项目已经进行很久了，github上有持续更新，目前平台上的各种功能已经比较完善了。介绍了多种openedx的安装方法：git安装、ova镜像、run文件、官方box文件 git安装的方法 由于外网的原因，openedx的直接安装一般不太可能成功，优先选择手动安装的方法。 根据你的ubuntu系统来选择安装（目前支持ubuntu12.04LTS, ubuntu16.04LTS两个版本，配置要求不同（内存，最低硬盘容量要求）），可以参考gihub上的 edx/configuration 、edx/edx-platform 这两个开源项目 本人使用了12.04的安装平台，具体的安装方法可以参考：https://openedx.atlassian.net/wiki/display/OpenOPS/Native+Open+edX+Ubuntu+12.04+64+bit+Installation 这里罗列了一些手动安装过程中会出现的一些问题由于接触ubuntu不多，需要明白一个原则：每一步的命令执行没有问题之后，才能接着执行下面的命令安装过程中的问题，主要集中在检查配置文件，向git请求下载的过程之中1sudo ansible-playbook -c local ./edx_sandbox.yml -i &quot;localhost,&quot; 整理了一些错误报告（1） cannot import name _unicodepy对应的mongo数据库没有安装成功（python链接mongo数据库的中间键）1sudo apt-get install python-pymongo 已经安装了还是不行，选择安装最新版本的12sudo pip uninstall pymongo bsonsudo pip install pymongo --upgrade （2）pip软件包的警告1SNIMissingWarning: An HTTPS request has been made,.... see https://urllib3.readthedocs.org/en/latest/security.html#snimissingwarning. 安装依赖包1pip install pyopenssl ndg-httpsclient pyasn1 （3）用户权限问题1The directory &apos;/home/onemind/.cache/pip/http&apos; or its parent directory is not owned by the current user and the cache has been disabled. 可以参考linux权限辅助的处理方案 （4）安装mongo服务器时候1&quot;msg&quot;: &quot;Failed to update apt cache.&quot; 删除 /var/lib/apt/lists下所有缓存文件1sudo rm -rf * 更新apt列表1sudo apt-get update ova镜像挂载 挂载打包好的opeedx OVA镜像，可以免除繁琐的下载过程，但需要后期的配置（将镜像文件挂载到WMware中去，后期主要是配置网络IP） edustack网站给出的指引方案 可以参考：http://edustack.org/manual/edx/open-edx-ebook%E4%B8%AD%E6%96%87%E7%89%88/ run文件安装 bitnami开源项目托管网站打包了openedx的开源项目。当然，打包的项目不可能是最新的开发版了。 附上地址：https://bitnami.com/stack/edx 该文件是在ubuntu系统下进行安装： （1）安装openedx（添加权限后安装）12sudo chmod 755 bitnami-edx-20160414-4-linux-x64-installer.run./bitnami-edx-20160414-4-linux-x64-installer.run （2）安装配置安装过程中，需要配置ip，托管邮箱等（用以发送邮件验证），如果在图形界面中配置，则很简单。附上dos界面的配置： （3）修改主机名/ip地址1sudo installdir/apps/edx/bnconfig --machine_hostname NEW_DOMAIN(服务器ip) （4）备份12$ /home/azureuser/edx-20160414-4/ctlscript.sh stop$ tar -pczvf application-backup.tar.gz /home/azureuser/edx-20160414-4 （5）启动openedx1$ /home/azureuser/edx-20160414-4/ctlscript.sh start box文件github上放出的最新的box镜像 环境要求：vagrant+vitualbox 参考地址：http://edx.readthedocs.io/projects/edx-installing-configuring-and-running/en/latest/installation/index.html 安装vagrant(ubuntu/windows平台都有)，安装完成后自动，重启会自动完成所有的路径配置 操作流程新建一个文件夹，导入对应的box文件12vagrant box add (name) (file_directory)vagrant init (name) 修改配置文件（1）打开网络地址（包括本地网络和外网地址）12config.vm.network &quot;forwarded_port&quot;, guest: 80, host: 8080config.vm.network &quot;private_network&quot;, ip: &quot;192.168.33.10&quot; （2）配置映射文件本地文件夹 —&gt; 映射文件夹（虚拟机中对应的文件夹，会自动生成），配置中以当前文件目录为起始点1config.vm.synced_folder &quot;./data&quot;, &quot;/vagrant_data&quot; 启动、结束、重启123vagrant upvagrant haltvagrant reload]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[异常处理方法(python)]]></title>
      <url>%2F2017%2F01%2F07%2Ftry%E8%AF%AD%E5%8F%A5%2F</url>
      <content type="text"><![CDATA[try/catch语句用以处理python编程过程中可能出现的异常情况 python是一个高度应用（解释型）语言，在运用过程中的种种疏漏有时难以避免 [捕获所有异常]12345try: a=b b=c except Exception,e: print Exception,":",e [采用traceback模块查看异常]1234567891011121314151617181920#引入python中的traceback模块，跟踪错误import traceback try: a=b b=c except: traceback.print_exc()#也可以将异常存储到日志文件中去import tracebacktry: a=b b=c except: f=open("c:log.txt",'a') traceback.print_exc(file=f) f.flush() f.close() [采用sys模块回溯最后的异常]12345678#引入sys模块import sys try: a=b b=c except: info=sys.exc_info() print info[0],":",info[1]]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python中序列相关的内建函数]]></title>
      <url>%2F2016%2F12%2F27%2Fpython%E5%86%85%E5%BB%BA%E5%87%BD%E6%95%B0%E5%BA%8F%E5%88%97%E7%AF%87%2F</url>
      <content type="text"><![CDATA[python常见的与序列相关的内建函数有4种：sorted()、reversed()、enumerate()、zip()。其中sorted()、reversed()是返回的一个列表对象，而enumerate()、zip()返回的是一个迭代器。 给出代码测试12345678&gt;&gt;&gt; type(sorted(s)) &lt;type 'list'&gt;&gt;&gt;&gt; type(zip(s)) &lt;type 'list'&gt;&gt;&gt;&gt; type(reversed(s)) &lt;type 'listreverseiterator'&gt;&gt;&gt;&gt; type(enumerate(s))&lt;type 'enumerate'&gt; zip()函数zip([seql, …])接受一系列可迭代对象作为参数，将对象中对应的元素打包成tuple，然后返回由这些tuples组成的list。若传入参数的长度不等，则返回list的长度和参数中长度最短的对象相同。两个字符串载匹配是按少匹配，zip与*组合，形成一个行列变换。 基本用法123456789s1=[1,2,3]s2=[4,5,6,7]&gt;&gt;&gt; result=zip(s1.s2)result:[(1,4),(2,5),(3,6)]&gt;&gt;&gt; zip(*result)[(1,2,3),(4,5,6)]&gt;&gt;&gt; map(list, zip(*a))[[1,2,3],[4,5,6]] 进阶用法1. zip打包解包列表和倍数1234567a = [1, 2, 3]b = ['a', 'b', 'c']&gt;&gt;&gt; z = zip(a, b)[(1, 'a'), (2, 'b'), (3, 'c')]&gt;&gt;&gt; zip(*z)[(1, 2, 3), ('a', 'b', 'c')] 2. 使用zip合并相邻的列表项1a = [1, 2, 3, 4, 5, 6] （1）’*’ 方法定义多个list12&gt;&gt;&gt; zip(*([iter(a)] * 2))[(1, 2), (3, 4), (5, 6)] zip(([iter(a)] * k))输出结果为a的k个对象，并没有详细的记过，应该是一种特殊组合123group_adjacent = lambda a, k: zip(*([iter(a)] * k))&gt;&gt;&gt; group_adjacent(a, 3)[(1, 2, 3), (4, 5, 6)] （2）使用多重循环可以使用多重循环达到一样的效果12&gt;&gt;&gt; zip(a[::3], a[1::3], a[2::3])[(1, 2, 3), (4, 5, 6)] 123group_adjacent = lambda a, k: zip(*(a[i::k] for i in range(k)))&gt;&gt;&gt; roup_adjacent(a, 3)[(1, 2, 3), (4, 5, 6)] 3. 使用zip和iterators生成滑动窗口 (n -grams)1234567891011121314from itertools import islicedef n_grams(a, n): z = (islice(a, i, None) for i in range(n)) return zip(*z)a = [1, 2, 3, 4, 5, 6]&gt;&gt;&gt; n_grams(a, 3)[(1, 2, 3), (2, 3, 4), (3, 4, 5), (4, 5, 6)]&gt;&gt;&gt; n_grams(a, 2)[(1, 2), (2, 3), (3, 4), (4, 5), (5, 6)]&gt;&gt;&gt; n_grams(a, 4)[(1, 2, 3, 4), (2, 3, 4, 5), (3, 4, 5, 6)] 4. 使用zip反转字典12345678m = &#123;'a': 1, 'b': 2, 'c': 3, 'd': 4&#125;&gt;&gt;&gt; m.items()[('a', 1), ('c', 3), ('b', 2), ('d', 4)]&gt;&gt;&gt; zip(m.values(), m.keys())[(1, 'a'), (3, 'c'), (2, 'b'), (4, 'd')]&gt;&gt;&gt; mi = dict(zip(m.values(), m.keys()))&#123;1: 'a', 2: 'b', 3: 'c', 4: 'd'&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[基本数据结构]]></title>
      <url>%2F2016%2F12%2F20%2Fpython%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%95%B4%E7%90%86%2F</url>
      <content type="text"><![CDATA[常用的几个数据结构有: list[], tumple(), dict{}, set()，主要对常用的方法和一些细节进行备注 listpython二维数组二维数组的定义如下操作只是定义了3个指向list的引用，指向一个地址空间1234mylist= [[]]*3&gt;&gt;&gt;mylist[0].append(3)mylist=[[3][3][3]] 用如下的方式定义：1mylist=[[0]*n, [0]*n, ...] tumple dict 种类包括有序字典和无序字典 有序字典12import collectionsordr_dic=collections.OrderedDict 字典的迭代字典是无序的，字典默认读取的是字典元素的key12for key in dic: print key:dic[key] （1）iter(dict)在有序字典中，读取字典的全部属性值直接读取对应的key和value （2）下标读取字典的下标可以也是元组（list不行，key要求不变性）12for e in dict: print e[0],e[1] （3）分开读取values(), keys()实质上是把一个 dict 转换成了包含 value/key 的list。itervalues(), interkeys()直接从dict中取出数据，返回的是 对象 （4）dict.items()/ dict.iteritems()读取[index,element]组合，前者会先生成一个list set]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python中数据/参数读取]]></title>
      <url>%2F2016%2F12%2F06%2Fpython%E8%AF%BB%E5%8F%96%E6%8E%A7%E5%88%B6%E5%8F%B0%E6%95%B0%E6%8D%AE%2F</url>
      <content type="text"><![CDATA[主要区分参数读取和数据读取这两个概念，一个作用于程序的执行环境，一个作用于程序代码本身。读取参数：在程序编译之前，所设置的对应参数读取数据：执行在程序编译之后，默认接受用户在控制台通过键盘键入的数据 参数读取在pycharm中，通过配置运行环境，预设好参数（通过argv[]获取参数） 12345import sys# "i=0"时，表示代码本身文件路径，"id =1"代表第一个参数sys.argv[i]# 第i个参数（j--&gt;k位）sys.argv[i][j:k] 控制台[raw_input/input] raw_input([prompt])函数从标准输入读取一个行，并返回一个字符串（去掉结尾的换行符）将每行的输入看作一个字符串来处理，需要对数据进行处理 1234#如果希望读取出in型数据，则需要进行转化map(int, raw_input().split())int(raw_input().split()[0]) input([prompt]) 函数与raw_input([prompt]) 函数基本类似可以接收一个Python表达式作为输入（希望接受的是一个合法的python表达式） 1234s=input()# string，需要将输入通过引号括起来，不然会报SyntaxError错误# 输入数字会自动识别为int/float型# 输入正确的表达式，会给出执行后的结果 [sys.stdin] readline()标准输入读取一行，但是会读取换行符’\n’ 12import sysstr= sys.stdin.readline() realines()读取所有的输入（读取了所有行的输入信息），有时候我们需要处理每行读取后的’\n’ 123456#fist methodfor e in sys.stdin.readlines(): e=e.strip()#second methodfor e in sys.stdin.read().splitlines(): pass]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[csv文件到Mysql数据库]]></title>
      <url>%2F2016%2F12%2F05%2Fcsv_Mysql%2F</url>
      <content type="text"><![CDATA[如何获取想要的数据，并进行结构化，这是数据挖掘的基础（也是后期数据的基础）。之后，如何对这些数据进行清洗（去重，补全），如何进行存储？ 获取数据一般有如下几个方法： 1.使用爬虫获取网络上对应的数据并进行结构化处理，写到csv、txt文件中去 2.网页上下载对应的csv文件或者txt文件（某些网站提供了源数据） 3.某些网站的web api，可以直接获得了比较清晰的json数据流了豆瓣图书API：https://developers.douban.com/代码示例：获取小王子图书的主页信息 123import requestsr = requests.get('https://api.douban.com/v2/book/1084336')print(r.text) 4.一些语料库的可以直接使用NTTK语料库: http://www.nltk.org/ execle/text -&gt; csv 在mysql数据中，文本格式一般是utf-8。而一般在处理时候，得到的execle/text文档是gbk，gb2312,所以在做csv转换时一定要注意编码问题。 基于python2.x字符串在Python内部的表示是unicode编码。因此，在做编码转换时，通常需要以unicode作为中间编码，最后转成utf-8写入csv。 （1）文本转码 123file = open('path/to/file', 'r')for content in file: data = content.decode('gbk').encode('utf-8') （2）替换整个python的脚本编码 123import sysreload(sys) sys.setdefaultencoding('utf-8') 基于python3.xpython3.x的默认编码为utf-8，省去了开头的脚本转码过程在3.x中 open方法也得到了更新，可以在创建文档时指定编码1234input_txt = open('input.txt', 'w', encoding='gbk')input_csv = pd.read_csv('input.csv', encoding='gbk')output = open('core_lab.csv', 'w', encoding='utf-8') 判断一个文档的编码调用chardet包，会读取字符流中所有的内容，然后给出置信度最好的编码名称高级用法: http://chardet.readthedocs.io/en/latest/usage.html1234import chardetresult = chardet.detect(data) return result['encoding'] mysql数据库介绍 很多情况下，获得结构化数据后，需要存入数据库后调用，并进行动态化的展示。通常使用MYsql数据库，python与mysql数据库的连接是通过一个统一的规范的接口实现的（DB APT），需要安装对应的中间层包。 中间层官方提供的包过于老了，只有32位，且不支持python3.x：https://pypi.python.org/pypi/MySQL-python/1.2.5 提供一个多所大学联合提供的python扩展包下载地址，在这儿能找到64位的：http://www.lfd.uci.edu/~gohlke/pythonlibs/#mysql-python 逻辑结构 中间层（DB API） Python应用程序（包含sql）与底层数据库之间的连接，需要提供接口，而各个厂商提供自己的接口程序（Mysql,oracle…） 基于这个原因，设计了python访问数据库的统一规范接口(DB API)，这是基于ORM模型的思想。 参考文档：https://www.python.org/dev/peps/pep-0249/ ，整个接口包含多个对象内容。 mysql数据库的连接基本方法 安装完对应的mysql-python模块后，使用传统的方式连接数据库 连接数据库12345678910111213def set_connect(): config = &#123; 'host': 'localhost', 'port': 3306, 'user': 'root', 'passwd': 'root', 'db': 'professor', 'charset': 'utf8' &#125; conn = MySQLdb.connect(**config) return conn 增删改查执行增删改查的工作，需要设置游标，改变数据库的操作（插入、更新）则需要提交改变具体的sql语句去查对应的语法，下面给出一些示例1234567891011121314# define the coursorcursor = conn.cursor()# query data by conditionssql = "SELECT * FROM table \ WHERE name = '%s' AND age = '%d'" % (name, age)# update the datasql = "update table set web = '%s'\ WHERE Name = '%s'" % (web, name)# excute sql and commit it to databasecursor.execute(sql)conn.commit()]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[简易爬虫]]></title>
      <url>%2F2016%2F12%2F05%2F%E9%9D%99%E6%80%81%E7%BD%91%E9%A1%B5%E7%88%AC%E8%99%AB%E6%8A%93%E5%8F%96%2F</url>
      <content type="text"><![CDATA[爬虫是一种自动抓取互联网信息的程序，可以取互联网中有价值的数据。传统的人工方式，通过URL之间的联系来获取网页，覆盖面小，效率低下。开发一个最简易的爬虫（爬去静态网页，没有使用框架，效率很低，仅作为对爬虫的入门了解） 源码地址：https://github.com/reflexzj/simple-crawler.git 一、概述简单的爬虫组成 URL管理器 网页下载器 网页解析器这三个部分组成 给出三个部件协同工作流程 二、url管理器为什么需要urlManager这个组件？ 防止重复抓取、循环抓取（最坏情况：两个URL之间相互指向） 添加新的URL时需要判断 URL管理的基本功能范围 实现方案使用集合将待爬取的url放入集合中 python中的set()方法可以去除重复元素 关系型数据库 通过表和相应的标志位来实现这个功能 缓存数据库 example: redis 使用两个数据的方案（大型公司的选用方式） 个人可以使用电脑的内存或者是关系型数据库 三、网页下载器 将互联网上URL对应的网页下载到本地的工具 工具urllib2Python官方的基础模块： 支持直接的URL下载 向网页提交一些需要用户输入的数据 需要登陆网页的cookie处理 代理访问功能 requests 第三方插件，提供更为强大的功能 urllib2用法1. 将URL传送给urllib2.urlopen()方法12345678910import urllib2# 直接请求response = urllib2.urlopen('http://www.baidu.com')# 获取状态码，如果是200，则表示获取成功print response.getcode()# 读取内容contt = response.read() 2. 添加data、http header 增强处理：向服务器提交需要用户输入的数据 将url、data、header等信息传给request，将request对象作为参数发送网页请求 12345678910111213import urllib2# 创建request对象request = urllib2.request(url)# 添加数据（'a'这个数据项的值为'1'）request.add_data('a','1')# 添加http的header（伪装成Mozilla浏览器）request.add_header('USer-Agent','Mozilla/5.0')# 发送请求获取结果response = urllib2.urlopen(request) 3. 添加特殊场景的处理 登陆访问：HTTPCookieProcesser 代理访问：ProxyHandler 协议加密：HTTPSHandler URL之间相互跳转：HTTPRedirectHandler ….. example：增强cookie的处理 12345678910111213import urllib2, cookielib# 创建cookie容器cj = cookielib.CookieJar()# 将cookieJar作为一个参数生成对应的Handler，将此Handler传给Openeropener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))# 增强处理器，为urllib2安装openerurllib2.install_opener(opener)# 此时使用带有cookie的urllib2来访问网页response = urllib2.urlopen("http://www.baidu.com/") 四、网页解析器种类介绍1.模糊匹配解析 一般使用正则表达式 将整个网页文档当成一个字符串，使用模糊匹配的方式提取数据 直观，面对复杂内容，效率底下 2.结构化解析 将整个网页文档下成一个文档对象模型树DOM(document Object Model) 解析工具1.html.parser 自带的解析工具2.Beautiful Soup 可已使用html.parser以及Ixml来作为其的解析器3.Ixml Beautiful Soup安装 文档参考地址：https://www.crummy.com/software/BeautifulSoup/bs4/doc/ 在windows环境下通过python工具包自带的pip工具来安装，pip程序在Scripts文件中，检查是否安装12cd python_27\Scriptspip install beautifulsoup4 用法 原理:根据下载好的Html网页创建一个BeatifulSoup对象，文档则会变为相应的DOM树 代码实现（1）创建BeautifulSoup对象 根据HTML网页字符串创建BeautifulSoup对象，对象中的参数分别表示为HTML文档字符串、HTML解析器、指定文档的编码123from bs4 import BeautifulSoupsoup= BeautifulSoup(html_doc,'html.parser',from_encoding='utf8') （2）搜索节点（find_all,find） 这两个方法具有相同的参数：d_all(name,attrs,string) find_all() 搜索所有满足条件的节点 find() 搜索第一个满足条件的节点 -方法中的三个参数代表搜索的三个方向，分为名称、属性、内容 123456789# 查找所有标签为a的节点soup.find_all('a')# 查找所有标签为啊，链接符合/view/123.htm形式的节点(也可以传入正则表达式)soup.find_all('a',href='/view/123.htm')soup.find_all('a',href=re.compile(r'/view/\d+\.htm'))# 查找所有标签为div，class为abc，文字为Python的节点(class为python关键字，加下划线避免冲突)soup.find_all('div',class_='abc',string='Python') （3）访问节点信息 12345678910# 得到节点：&lt;a href='1.html'&gt;Python&lt;/a&gt;# 获取查找到的节点标签名称node.name# 获取查找到的a节点的href属性node['href']# 获取查找到的a节点的链接文字node.get_text()]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[算法整理(c++)]]></title>
      <url>%2F2016%2F10%2F13%2Fc_c%2B%2B_%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86%2F</url>
      <content type="text"><![CDATA[持续整理，从简单的算法开始。与原来记忆中的知识体系形成一个对照，帮助复习和记忆 字符串字符串逆置统计字符串的长度length()/size()其实这两个的功能是一样的，c++更倾向于用size(),用于STL中除了考虑时间复杂度，空间复杂度的问题需不需要考虑？（这可是c++啊，新建一个字符串，逆序存入的方法，真是给大学算法老师丢脸了。）不出所料地，出现了 Runtime Error 的错误。 1234567891011class Solution &#123;public: string reverseString(string s) &#123; int len=s.size(); string result; for(int i=len-1; i&gt;=0; i--)&#123; result[len-i-1]=s[i]; &#125; return result; &#125;&#125;; 给出最优的方案：从两头开始向中间移动指针1234567891011class Solution &#123;public: string reverseString(string s) &#123; int i = 0, j = s.size() - 1; while(i &lt; j)&#123; swap(s[i++], s[j--]); &#125; return s; &#125;&#125;; 位运算汉明距离 汉明距离：两个数进行二进制转换后，不同的位数 思路：执行异或操作之后，n转化为二进制格式，其中”1”的个数即为汉明距离考虑运算：n&amp;n-1， 每次操作都会消除一个低位的”1”,借位会消除所在位置的”1”123456n=x^ywhile(n)&#123; ++flag; n &amp;= n - 1;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Main主函数]]></title>
      <url>%2F2016%2F10%2F05%2Fmain%E5%87%BD%E6%95%B0%E5%88%86%E6%9E%90%2F</url>
      <content type="text"><![CDATA[主要讨论主函数中 name = ‘main‘ 写法意义 [给一个模块]12345#module.pydef main(): print "we are in %s"%__name__if __name__ == '__main__': main() 这个函数定义了一个main函数，执行后打印出”we are in main“,说明if语句中的内容被执行了，调用了main() [从另外一个模块中调用main()方法]123#anothermodle.pyfrom module import mainmain() 执行的结果是：we are in module 没有显示”we are in main“,也就是说模块name = ‘main‘ 下面的函数没有执行。 分析直接执行某个.py文件的时候，该文件中那么”name == ‘main‘“是True,但是如果从另外一个.py文件通过import导入该文件的时候，这时name的值就是这个py文件的名字而不是main。 这个功能还有一个用处：调试代码的时候，在”if name == ‘main‘“中加入一些的调试代码，可以让外部模块调用的时候不执行调试代码，但是如果想排查问题的时候，直接执行该模块文件，调试代码能够正常运行。]]></content>
    </entry>

    
  
  
</search>
